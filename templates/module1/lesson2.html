{% extends 'base.html' %}

{% block title %}Module 1: Lesson 2 - Data Preprocessing and Feature Engineering{% endblock %}

{% block content %}
<h2>Lesson 2: Data Preprocessing and Feature Engineering</h2>

<p>In this lesson, we will explore the importance of data preprocessing and feature engineering in computational drug discovery. These techniques are crucial for preparing the data for machine learning models and improving their performance.</p>

<h3>Topics Covered</h3>

<ul>
    <li>Introduction to data preprocessing</li>
    <li>Handling missing data</li>
    <li>Data normalization and standardization</li>
    <li>Dealing with categorical variables</li>
    <li>Feature scaling techniques</li>
    <li>Feature selection methods</li>
    <li>Dimensionality reduction techniques (PCA, t-SNE)</li>
    <li>Creating new features through feature engineering</li>
</ul>

<h3>Learning Objectives</h3>

<p>By the end of this lesson, you will be able to:</p>

<ul>
    <li>Understand the importance of data preprocessing in machine learning.</li>
    <li>Handle missing data using various techniques such as imputation and deletion.</li>
    <li>Normalize and standardize data to improve model performance.</li>
    <li>Encode categorical variables for use in machine learning models.</li>
    <li>Apply feature scaling techniques to bring features to a similar scale.</li>
    <li>Perform feature selection to identify the most informative features.</li>
    <li>Use dimensionality reduction techniques to reduce the number of features.</li>
    <li>Create new features through feature engineering to capture domain-specific information.</li>
</ul>

<h3>Lesson Content</h3>

<h4>Introduction to Data Preprocessing</h4>

<p>Data preprocessing is the process of transforming raw data into a clean and structured format suitable for machine learning. It involves several steps, such as handling missing data, scaling features, and encoding categorical variables. Proper data preprocessing is essential for building accurate and reliable models.</p>

<h4>Handling Missing Data</h4>

<p>Missing data is a common problem in real-world datasets. There are various strategies for dealing with missing data:</p>

<ul>
    <li>Deletion: Remove samples or features with missing values.</li>
    <li>Imputation: Fill in missing values using techniques like mean, median, or mode imputation.</li>
    <li>Advanced imputation: Use machine learning algorithms like K-Nearest Neighbors (KNN) or Matrix Factorization to estimate missing values.</li>
</ul>

<h4>Data Normalization and Standardization</h4>

<p>Normalization and standardization are techniques used to bring features to a similar scale. This is important when features have different units or ranges.</p>

<ul>
    <li>Normalization: Scales the features to a fixed range, typically between 0 and 1.</li>
    <li>Standardization: Transforms the features to have zero mean and unit variance.</li>
</ul>

<h4>Dealing with Categorical Variables</h4>

<p>Categorical variables are non-numeric variables that represent categories or groups. Machine learning models typically require numerical inputs, so categorical variables need to be encoded. Common encoding techniques include:</p>

<ul>
    <li>One-Hot Encoding: Creates binary dummy variables for each category.</li>
    <li>Label Encoding: Assigns a unique numerical value to each category.</li>
    <li>Ordinal Encoding: Assigns numerical values based on the order or hierarchy of categories.</li>
</ul>

<h4>Feature Selection</h4>

<p>Feature selection is the process of identifying the most relevant and informative features for a machine learning model. It helps reduce dimensionality, improve model performance, and increase interpretability. Common feature selection methods include:</p>

<ul>
    <li>Univariate Selection: Evaluates each feature individually using statistical tests.</li>
    <li>Recursive Feature Elimination (RFE): Iteratively removes the least important features.</li>
    <li>L1 Regularization (Lasso): Performs feature selection by adding a penalty term to the model's objective function.</li>
</ul>

<h4>Dimensionality Reduction</h4>

<p>Dimensionality reduction techniques aim to reduce the number of features while retaining the most important information. They can be used for visualization, noise reduction, and computational efficiency. Popular dimensionality reduction techniques include:</p>

<ul>
    <li>Principal Component Analysis (PCA): Transforms the features into a lower-dimensional space while maximizing variance.</li>
    <li>t-Distributed Stochastic Neighbor Embedding (t-SNE): Visualizes high-dimensional data in a lower-dimensional space while preserving local structure.</li>
</ul>

<h4>Feature Engineering</h4>

<p>Feature engineering involves creating new features from existing ones to capture domain-specific information and improve model performance. It requires domain knowledge and creativity. Examples of feature engineering techniques in drug discovery include:</p>

<ul>
    <li>Calculating physicochemical properties of molecules.</li>
    <li>Creating fingerprints or structural features.</li>
    <li>Combining multiple features through mathematical operations.</li>
</ul>

<h3>Exercises</h3>

<ol>
    <li>Load a dataset with missing values and apply different imputation techniques to handle the missing data.</li>
    <li>Normalize and standardize a dataset containing features with different scales.</li>
    <li>Encode categorical variables in a dataset using one-hot encoding and label encoding.</li>
    <li>Perform feature selection using univariate selection and recursive feature elimination on a high-dimensional dataset.</li>
    <li>Apply PCA and t-SNE to reduce the dimensionality of a dataset and visualize the results.</li>
</ol>

<h3>Additional Resources</h3>

<ul>
    <li><a href="https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank">Scikit-learn Preprocessing Documentation</a></li>
    <li><a href="https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/" target="_blank">Feature Engineering Techniques</a></li>
    <li><a href="https://www.kaggle.com/learn/data-visualization" target="_blank">Kaggle Data Visualization Course</a></li>
</ul>

{% endblock %}