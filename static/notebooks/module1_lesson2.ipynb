{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Lesson 2: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore practical examples of data preprocessing and feature engineering techniques using Python libraries such as NumPy, Pandas, and Scikit-learn. We will cover the following topics:\n",
    "\n",
    "1. Handling missing data\n",
    "2. Data normalization and standardization\n",
    "3. Dealing with categorical variables\n",
    "4. Feature scaling\n",
    "5. Feature selection\n",
    "6. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before we start, make sure you have the following libraries installed:\n",
    "\n",
    "- NumPy\n",
    "- Pandas\n",
    "- Scikit-learn\n",
    "\n",
    "You can install them using the following command:\n",
    "\n",
    "```\n",
    "pip install numpy pandas scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with missing values\n",
    "data = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4],\n",
    "    'B': [5, np.nan, 7, 8],\n",
    "    'C': [9, 10, 11, np.nan]\n",
    "})\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nImputed Data:\")\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "# Normalize the data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Standardize the data using Z-score standardization\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(normalized_data)\n",
    "print(\"\\nStandardized Data:\")\n",
    "print(standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with categorical variables\n",
    "data = pd.DataFrame({\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red'],\n",
    "    'Size': ['Small', 'Medium', 'Large', 'Medium']\n",
    "})\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(data).toarray()\n",
    "\n",
    "# Perform label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "data['Color_Encoded'] = label_encoder.fit_transform(data['Color'])\n",
    "data['Size_Encoded'] = label_encoder.fit_transform(data['Size'])\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nOne-Hot Encoded Data:\")\n",
    "print(encoded_data)\n",
    "print(\"\\nLabel Encoded Data:\")\n",
    "print(data[['Color_Encoded', 'Size_Encoded']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Perform univariate feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "selected_features = selector.fit_transform(X, y)\n",
    "\n",
    "# Perform recursive feature elimination\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "selected_features_rfe = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Original Features:\")\n",
    "print(X)\n",
    "print(\"\\nSelected Features (Univariate):\")\n",
    "print(selected_features)\n",
    "print(\"\\nSelected Features (RFE):\")\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X)\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_result = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax1.scatter(pca_result[:, 0], pca_result[:, 1])\n",
    "ax1.set_title('PCA')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "\n",
    "ax2.scatter(tsne_result[:, 0], tsne_result[:, 1])\n",
    "ax2.set_title('t-SNE')\n",
    "ax2.set_xlabel('Dimension 1')\n",
    "ax2.set_ylabel('Dimension 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored various data preprocessing and feature engineering techniques using Python libraries. We covered handling missing data, data normalization and standardization, dealing with categorical variables, feature scaling, feature selection, and dimensionality reduction.\n",
    "\n",
    "These techniques are crucial for preparing data for machine learning models and improving their performance. By applying these techniques, we can ensure that our data is clean, structured, and relevant for the task at hand.\n",
    "\n",
    "Feel free to experiment with different datasets and parameters to further solidify your understanding of these concepts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}